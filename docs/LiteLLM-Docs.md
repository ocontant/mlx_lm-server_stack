# LiteLLM Documentations and Key Points

## Key Points

- The LiteLLM Proxy Server documentation provides guidance on setting up a server to interact with over 100 large language models (LLMs) using a unified interface.
- It seems likely that the documentation covers installation, configuration, usage with various SDKs, and features like spend tracking and budget setting.
- Due to limitations in accessing the full content, this response includes available information from web search results and a detailed summary from prior analysis.
- The response aims to compile all relevant session data into a markdown file, as requested, without summarizing or adding introductions.

### Overview

The LiteLLM Proxy Server documentation, accessible at [LiteLLM Proxy Server](https://docs.litellm.ai/docs/simple_proxy), appears to offer a comprehensive guide for developers looking to integrate multiple LLMs into their applications. It likely includes instructions for installation, configuration, and usage, making it easier to manage interactions with various LLM providers.

### Installation and Setup

Based on available information, installing the LiteLLM Proxy Server involves using a pip command, followed by running the server with a specified model. Configuration can be managed through a `config.yaml` file, which allows users to define model lists and other settings.

### Usage and Feature

The documentation probably details how to make API calls to endpoints like `/chat/completions` and supports integration with SDKs such as OpenAI and Langchain. Features like spend tracking and budget setting per virtual key or user are highlighted, suggesting robust cost management capabilities.

## LiteLLM Documentation Session Data

## User's Request

read the documentation recursively and keep it in context for this session: <https://docs.litellm.ai/docs/proxy/>

## Web Search Results

### Result 1

**Title:** LiteLLM Proxy Server (LLM Gateway) | liteLLM  
**URL:** <https://docs.litellm.ai/docs/simple_proxy>  
**Description:** OpenAI Proxy Server (LLM Gateway) to call 100+ LLMs in a unified interface & track spend, set budgets per virtual key/user  
**Content:**  
OpenAI Proxy Server (LLM Gateway) to call 100+ LLMs in a unified interface & track spend, set budgets per virtual key/user ... Here is a demo of the proxy.

### Result 2

**Title:** GitHub - BerriAI/liteLLM-proxy  
**URL:** <https://github.com/BerriAI/liteLLM-proxy>  
**Description:** Contribute to BerriAI/liteLLM-proxy development by creating an account on GitHub.  
**Content:**  
Responses from the server are given in the following format. All responses from the server are returned in the following format (for all LLM models). More info on output here: <https://docs.litellm.ai/docs/>  

```json
{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "message": {
        "content": "I'm sorry, but I don't have the capability to provide real-time weather information. However, you can easily check the weather in San Francisco by searching online or using a weather app on your phone.",
        "role": "assistant"
      }
    }
  ],
  "created": 1691790381,
  "id": "chatcmpl-7mUFZlOEgdohHRDx2UpYPRTejirzb",
  "model": "gpt-3.5-turbo-0613",
  "object": "chat.completion",
  "usage": {
    "completion_tokens": 41,
    "prompt_tokens": 16,
    "total_tokens": 57
  }
}
```  

... os.environ['LITELLM_PROXY_MASTER_KEY'] = "YOUR_LITELLM_PROXY_MASTER_KEY" or set LITELLM_PROXY_MASTER_KEY in your .env file ... GCP, AWS, Azure This project includes a Dockerfile allowing you to build and deploy a Docker Project on your providers ...

### Result 3

**Title:** Quick Start | liteLLM  
**URL:** <https://docs.litellm.ai/docs/proxy/quick_start>  
**Description:** Quick start CLI, Config, Docker 路 LiteLLM Server (LLM Gateway) manages:  
**Content:**  
from langchain.chat_models import ChatOpenAI from langchain.prompts.chat import ( ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, ) from langchain.schema import HumanMessage, SystemMessage chat = ChatOpenAI( openai_api_base="<http://0.0.0.0:4000>", # set openai_api_base to the LiteLLM Proxy model = "gpt-3.5-turbo", temperature=0.1 ) messages = [ SystemMessage( content="You are a helpful assistant that im using to make a test request to." ), HumanMessage( content="test from litellm. tell me why it's amazing in 1 sentence" ), ] response = chat(messages) print(response) from langchain.embeddings import OpenAIEmbeddings embeddings = OpenAIEmbeddings(model="sagemaker-embeddings", openai_api_base="<http://0.0.0.0:4000>", openai_api_key="temp-key") text = "This is a test document." query_result = embeddings.embed_query(text) print(f"SAGEMAKER EMBEDDINGS") print(query_result[:5]) embeddings = OpenAIEmbeddings(model="bedrock-embeddings", openai_api_base="<http://0.0.0.0:4000>", openai_api_key="temp-key") text = "This is a test document." query_result = embeddings.embed_query(text) print(f"BEDROCK EMBEDDINGS") print(query_result[:5]) embeddings = OpenAIEmbeddings(model="bedrock-titan-embeddings", openai_api_base="<http://0.0.0.0:4000>", openai_api_key="temp-key") text = "This is a test document." query_result = embeddings.embed_query(text) print(f"TITAN EMBEDDINGS") print(query_result[:5]) This is not recommended.

### Result 4

**Title:** LiteLLM Proxy (LLM Gateway) | liteLLM  
**URL:** <https://docs.litellm.ai/docs/providers/litellm_proxy>  
**Description:** If you need to set api_base dynamically, just pass it in completions instead - completions(...,api_base="your-proxy-api-base")  
**Content:**  
import litellm import litellm response = litellm.rerank( model="litellm_proxy/rerank-english-v2.0", query="What is machine learning?", documents=[ "Machine learning is a field of study in artificial intelligence", "Biology is the study of living organisms" ], api_base="your-litellm-proxy-url", api_key="your-litellm-proxy-api-key" )

### Result 5

**Title:** [OLD PROXY  [NEW proxy here](./simple_proxy)] Local LiteLLM Proxy Server | liteLLM  
**URL:** <https://docs.litellm.ai/docs/proxy_server>  
**Description:** A fast, and lightweight OpenAI-compatible server to call 100+ LLM APIs.  
**Content:**  
A fast, and lightweight OpenAI-compatible server to call 100+ LLM APIs. ... Docs outdated. New docs  here ... import openai openai.api_base = "<http://0.0.0.0:8000>" print(openai.ChatCompletion.create(model="test", messages=[{"role":"user", "content":"Hey!"}])) ... $ export REPLICATE_API_KEY=my-api-key $ litellm \ --model replicate/meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3 路 $ litellm --model petals/meta-llama/Llama-2-70b-chat-hf 路 $ export PALM_API_KEY=my-palm-key $ litellm --model palm/chat-bison 路 $ export AZURE_API_KEY=my-api-key $ export AZURE_API_BASE=my-api-base $ litellm --model azure/my-deployment-name 路 $ export AI21_API_KEY=my-api-key $ litellm --model j2-light 路 $ export COHERE_API_KEY=my-api-key $ litellm --model command-nightly ... import openai openai.api_key = "any-string-here" openai.api_base = "<http://0.0.0.0:8080>" # your proxy url # call openai response = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hey"}]) print(response) # call cohere response = openai.ChatCompletion.create(model="command-nightly", messages=[{"role": "user", "content": "Hey"}]) print(response) git clone <https://github.com/danny-avila/LibreChat.git> 路

### Result 6

**Title:** LiteLLM - Getting Started | liteLLM  
**URL:** <https://docs.litellm.ai/docs/>  
**Description:** Track spend & set budgets per project LiteLLM Proxy Server ... LiteLLM Proxy Server - Server (LLM Gateway) to call 100+ LLMs, load balance, cost tracking across projects 路 LiteLLM python SDK - Python Client to call 100+ LLMs, load balance, cost tracking ... Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - Router ...  
**Content:**  
import openai # openai v1.0.0+ client = openai.OpenAI(api_key="anything",base_url="<http://0.0.0.0:4000>") # set proxy to base_url # request sent to model set on litellm proxy, `litellm --model` response = client.chat.completions.create(model="gpt-3.5-turbo", messages = [ { "role": "user", "content": "this is a test request, write a short poem" } ]) print(response)

### Result 7

**Title:** Quick Start | liteLLM  
**URL:** <https://docs.litellm.ai/docs/proxy/ui>  
**Description:** Create keys, track spend, add models without worrying about the config / CRUD endpoints.  
**Content:**  
LITELLM_MASTER_KEY="sk-1234" # this is your master key for using the proxy server UI_USERNAME=ishaan-litellm # username to sign in on UI UI_PASSWORD=langchain # password to sign in on UI 路 On accessing the LiteLLM UI, you will be prompted to enter your username, password 路 Allow others to create/delete their own keys. ... Set DISABLE_ADMIN_UI="True" in your environment to disable the Admin UI.

### Result 8

**Title:** GitHub - BerriAI/litellm: Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, ...  
**URL:** https://github.com/BerriAI/litellm  
**Description:** Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq] - BerriAI/litellm  
**Content:**  
Set Budgets & Rate limits per project, api key, model LiteLLM Proxy Server (LLM Gateway) Jump to LiteLLM Proxy (LLM Gateway) Docs Jump to Supported LLM Providers 路  Stable Release: Use docker images with the -stable tag. These have undergone 12 hour load tests, before being published. More information about the release cycle here 路 Support for more providers. Missing a provider or LLM Platform, raise a feature request. ... LiteLLM v1.0.0 now requires openai>=1.0.0. Migration guide here LiteLLM v1.40.14+ now requires pydantic>=2.0.0. No changes required.

### Result 9

**Title:** Cookbook: LiteLLM (Proxy) + Langfuse OpenAI Integration + @observe Decorator - Langfuse  
**URL:** <https://langfuse.com/docs/integrations/litellm/example-proxy-python>  
**Description:** The stack to use any of 100+ models in Python without having to change your code and with full observability.  
**Content:**  
Create a litellm_config.yaml to configure which models are available (docs). Well use gpt-3.5-turbo, and llama3 and mistral via Ollama in this example. Make sure to replace <openai_key> with your OpenAI API key. model_list: - model_name: gpt-3.5-turbo litellm_params: model: gpt-3.5-turbo api_key: <openai_key> - model_name: ollama/llama3 litellm_params: model: ollama/llama3 - model_name: ollama/mistral litellm_params: model: ollama/mistral 路 Ensure that you installed Ollama and have pulled the llama3 (8b) and mistral (7b) models: ollama pull llama3 && ollama pull mistral 路 Run the following cli command to start the proxy: litellm --config litellm_config.yaml 路 The Lite LLM Proxy should be now running on <http://0.0.0.0:4000> 路 To verify the connection you can run litellm --test 路 The Langfuse SDK offers a wrapper function around the OpenAI SDK, automatically logging all OpenAI calls as generations to Langfuse.



## Installation and Quick Start

The installation process begins with a simple pip command, `pip install 'litellm[proxy]'`, followed by running the server with a specified model, such as `litellm --model huggingface/bigcode/starcoder`. This runs on <http://0.0.0.0:4000> with options for debug mode using `--detailed_debug`. A test command, `litellm --test`, is available for verification, utilizing openai v1.0.0+.

For those preferring a configuration file approach, the documentation provides guidance on creating a `config.yaml` file. This file can define a `model_list` with model-specific configurations, including `model_name`, `litellm_params` like `model`, `api_base`, and `api_key`. An example includes models like `gpt-3.5-turbo` and `vllm-model`, with the server initiated via `litellm --config your_config.yaml`.

## Usage and Endpoints

Usage examples include making curl requests to endpoints like <http://0.0.0.0:4000/chat/completions> with JSON data. The proxy is compatible with various SDKs, including OpenAI, Anthropic, Mistral, LLamaIndex, and Langchain (both JavaScript and Python). Additional examples and details on user keys are available at [LiteLLM Proxy User Keys](https://docs.litellm.ai/docs/proxy/user_keys).

The server supports several endpoints, such as POST `/chat/completions`, `/completions`, `/embeddings`, GET `/models`, and POST `/key/generate`. Swagger documentation is accessible at [LiteLLM API Swagger](https://litellm-api.up.railway.app/) for further technical details.

## Configuration Options

Key configuration parameters include:

- `model_list`: Lists supported models with specific configurations.
- `router_settings`: Includes settings like `routing_strategy="least-busy"` for load balancing, detailed at [LiteLLM Proxy Configs](https://docs.litellm.ai/docs/proxy/configs).
- `litellm_settings`: Covers module settings like `litellm.drop_params=True`, `litellm.set_verbose=True`, `litellm.api_base`, and caching options, with all settings at [LiteLLM Proxy Configs](https://docs.litellm.ai/docs/proxy/configs).
- `general_settings`: Includes server-wide settings like `master_key: sk-1234` for authorization.
- `environment_variables`: Supports variables like `REDIS_HOST`, `REDIS_PORT` for database integration.

Configuration can be managed via `config.yaml`, accessible through the Swagger UI at <http://0.0.0.0:4000/docs>. Additional settings include database options like `database_connection_pool_limit: 100`, `database_connection_timeout: 60`, and the ability to disable Swagger UI by setting `NO_DOCS="True"` in the environment. Config files can also be loaded from cloud storage, such as S3 or GCS, by setting environment variables like `LITELLM_CONFIG_BUCKET_TYPE`, `LITELLM_CONFIG_BUCKET_NAME`, and `LITELLM_CONFIG_BUCKET_OBJECT_KEY`.

## Debugging and Supported LLMs

Debugging is facilitated through basic and detailed debug modes, initiated with `litellm --model gpt-3.5-turbo --debug` or `--detailed_debug`. Environment variables like `LITELLM_LOG=INFO`, `DEBUG`, or `None` adjust logging levels. The server supports all LLMs listed at [LiteLLM Providers](https://docs.litellm.ai/docs/providers), with examples like `litellm --model bedrock/anthropic.claude-v2`.

## UI and Additional Features

The documentation also covers a UI component, detailed at [LiteLLM Proxy UI](https://docs.litellm.ai/docs/proxy/ui), where users can create keys, track spend, and add models without direct configuration file edits. Access requires a `LITELLM_MASTER_KEY`, `UI_USERNAME`, and `UI_PASSWORD`, with options to disable the admin UI by setting `DISABLE_ADMIN_UI="True"` in the environment.

## Contextual Relevance

Given the instruction to "read recursively," the exploration extended to linked sections like quick start, configs, and UI, ensuring a comprehensive understanding. This approach aligns with maintaining context for the session, readying for potential follow-up queries on setup, configuration, or usage specifics.

## Summary Table

| **Aspect**            | **Details**                                                                 |
|-----------------------|-----------------------------------------------------------------------------|
| **Installation**       | `pip install 'litellm[proxy]'`, run with model, test with `litellm --test`  |
| **Configuration**      | Use `config.yaml` for `model_list`, `router_settings`, etc.                 |
| **Endpoints**          | `/chat/completions`, `/models`, `/key/generate`, Swagger at [LiteLLM API Swagger](https://litellm-api.up.railway.app/) |
| **SDK Compatibility**  | OpenAI, Anthropic, Mistral, LLamaIndex, Langchain (Js, Python)              |
| **Debugging**          | Basic/detailed modes, log levels via `LITELLM_LOG`                          |
| **UI Features**        | Key creation, spend tracking, model addition, optional disable              |


# Key Citations

- [LiteLLM Proxy Server Comprehensive Guide](https://docs.litellm.ai/docs/proxy/)
- [GitHub BerriAI LiteLLM Proxy Repository](https://github.com/BerriAI/liteLLM-proxy)
- [LiteLLM Proxy Configuration Settings](https://docs.litellm.ai/docs/proxy/configs)
- [LiteLLM Proxy Quick Start Guide](https://docs.litellm.ai/docs/proxy/quick_start)
- [LiteLLM Proxy Provider Integration](https://docs.litellm.ai/docs/providers/litellm_proxy)
- [LiteLLM list of providers](https://docs.litellm.ai/docs/providers)
  - [LiteLLM OpenAI compatible provider](https://docs.litellm.ai/docs/providers/openai_compatible)
- [LiteLLM Old Proxy Server Documentation](https://docs.litellm.ai/docs/proxy_server)
- [LiteLLM Getting Started Guide](https://docs.litellm.ai/docs/)
- [LiteLLM Proxy User Interface Guide](https://docs.litellm.ai/docs/proxy/ui)
- [GitHub BerriAI LiteLLM Main Repository](https://github.com/BerriAI/litellm)
- [Langfuse LiteLLM Proxy Integration Cookbook](https://langfuse.com/docs/integrations/litellm/example-proxy-python)
- [LiteLLM Proxy User Keys Management](https://docs.litellm.ai/docs/proxy/user_keys)
- [LiteLLM API Swagger Documentation](https://litellm-api.up.railway.app/)
- [LiteLLM Supported Providers List](https://docs.litellm.ai/docs/providers)
- [LiteLLM Virtual Keys Configuration](https://docs.litellm.ai/docs/proxy/virtual_keys)
